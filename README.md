## ğŸ“Š Project Overview
This project, "Hand Gesture Recognition System," focuses on developing a real-time gesture-based communication system using MediaPipe, OpenCV, and LSTM Neural Networks. The system aims to enhance human-computer interaction by accurately recognizing hand gestures, particularly for speech-impaired individuals. By leveraging AI-driven methodologies, the project facilitates gesture-to-text translation, improving accessibility and real-time communication.

## ğŸ‘¥ Team Members
Soham Vasudeo
Om Agrawal
Vanshaj Ajmera
Shardul Patki
Ritika Shetty
Course: Machine Learning Project 2022-23
Institution: NMIMS University

## ğŸ¯ Objectives
The objective of this project is to create a robust real-time hand gesture recognition system that can accurately interpret predefined gestures and expand to accommodate new gestures for various applications.

Key Goals:
âœ” Enable gesture-based communication for speech-impaired individuals.
âœ” Implement a real-time detection system with high accuracy and low latency.
âœ” Develop a scalable and adaptable model to recognize additional gestures.
âœ” Enhance human-computer interaction through gesture-controlled systems.

## ğŸ—‚ Data Understanding
The project leverages gesture-based image data to train and optimize the model for accurate recognition.

Dataset Components:
ğŸ“Œ Hand Gesture Images: Captured using MediaPipe Hands for training the model.
ğŸ“Œ Keypoints Extraction: 21 hand landmarks recorded for precise motion tracking.
ğŸ“Œ Predefined Gestures: The system recognizes five primary gestures:

Hello ğŸ–
I Love You ğŸ¤Ÿ
Thank You âœ‹
One â˜
Victory âœŒ

## ğŸ“Œ Data Storage: Organized in labeled directories for training, validation, and testing.
Challenges Addressed:
ğŸš§ Variations in Lighting & Backgrounds â€“ Improved model robustness to different environments.
ğŸš§ Real-Time Processing Constraints â€“ Optimized inference time for faster recognition.
ğŸš§ Multi-Gesture Recognition â€“ Designed the model to learn new gestures dynamically.

## âš™ï¸ Data Preparation
Data Cleaning & Processing:
ğŸ”¹ Removed noisy and distorted images.
ğŸ”¹ Standardized gesture images to a consistent resolution.
ğŸ”¹ Applied image augmentation (rotation, scaling) to improve model generalization.

Feature Engineering:
ğŸ”¹ Extracted hand keypoints (x, y, z coordinates) from video frames.
ğŸ”¹ Converted gesture data into structured NumPy arrays for training.

Data Splitting:
ğŸ“Œ Training Set â€“ 70%
ğŸ“Œ Validation Set â€“ 15%
ğŸ“Œ Testing Set â€“ 15%

## ğŸ§  Modeling
The project utilizes deep learning models to classify hand gestures based on extracted keypoints.

1. Hand Tracking & Feature Extraction:
âœ… MediaPipe Hands: Detects 21 keypoints for each hand.
âœ… OpenCV Preprocessing: Converts images into feature arrays for analysis.

2. Deep Learning Models for Gesture Classification:
ğŸ¤– LSTM (Long Short-Term Memory): Captures temporal dependencies in hand movements.
ğŸ¤– CNN (Convolutional Neural Networks): Extracts spatial patterns for accurate recognition.
ğŸ¤– ANN (Artificial Neural Networks): Classifies gesture patterns with optimized weight distribution.

3. Model Training & Fine-Tuning:
âœ” Hyperparameter Optimization: Adjusted learning rate, batch size, and epochs for best results.
âœ” Loss Function & Optimization: Categorical Cross-Entropy with Adam Optimizer.

## ğŸ“ˆ Model Evaluation
The performance of the gesture recognition system was evaluated using multiple metrics:

Classification Metrics:
ğŸ“Œ Accuracy: 87.5% on the test dataset.
ğŸ“Œ Precision & Recall: High precision ensures minimal false positives.
ğŸ“Œ F1-Score: Balanced assessment of precision and recall.

Inference Speed:
â³ Processing Time: ~3.81 ms per frame (real-time execution).

ğŸš€ Deployment Strategy
The gesture recognition system is integrated into a real-time application for gesture-to-text translation and smart control systems.

1. Infrastructure Setup:
â˜ Compatible with Edge & Cloud Systems â€“ Supports Raspberry Pi, AWS, Google Cloud.

2. Data Processing & Storage:
ğŸ”„ Automated Data Collection & Preprocessing for scalability.

3. Model Deployment:
âœ… Flask API / FastAPI â€“ Enables real-time communication.
âœ… TFLite for Mobile Deployment â€“ Optimized for mobile & embedded devices.

4. Continuous Monitoring & Updates:
ğŸ“Š Feedback Loop: Allows users to add new gestures dynamically.

## ğŸ“Š Key Insights and Results
ğŸ“Œ High Recognition Accuracy: 96.4% accuracy on test data.
ğŸ“Œ Fast Processing Speed: Real-time recognition at ~3.81 ms per frame.
ğŸ“Œ User-Friendly Interface: Seamless gesture-to-text translation for assistive communication.
ğŸ“Œ Expandable Gesture Set: New gestures can be added without retraining the entire model.

## ğŸ›  Tools & Technologies Used
Programming & Frameworks:
ğŸ–¥ Python, NumPy, OpenCV, TensorFlow, Keras

Machine Learning Models:
ğŸ§  LSTM, CNN, ANN

Hand Tracking & Data Processing:
ğŸ– MediaPipe Hands, OpenCV, NumPy

Visualization & Analytics:
ğŸ“Š Matplotlib, Seaborn, Power BI

Deployment & Integration:
ğŸŒ Flask, FastAPI, TensorFlow Lite (TFLite)

ğŸ“Š Visuals
ğŸ“¸ Dataset Samples: Hand gesture images captured for training.
![image](https://github.com/user-attachments/assets/a5ad327e-c702-466f-801f-9e95786f7465)
![image](https://github.com/user-attachments/assets/6fd2a07c-5338-4eca-b67e-815261f0477f)
![image](https://github.com/user-attachments/assets/a3594837-85c8-474b-a89f-4c76b695da6b)
![image](https://github.com/user-attachments/assets/597165f9-e38c-4170-89e6-7ede9ad19d47)
![image](https://github.com/user-attachments/assets/ab9470a6-c44e-4022-abd9-141ddaa04a83)

ğŸ“Š Model Performance Graphs: Accuracy, loss curves, and real-time testing results.
![image](https://github.com/user-attachments/assets/18206e2f-37a6-45d1-a14e-85785dcb8883)


## ğŸ† Achievements
ğŸ… Developed a fully functional real-time gesture recognition system.
ğŸ… Achieved 96.4% accuracy using deep learning models.
ğŸ… Successfully integrated MediaPipe for precise hand tracking.
ğŸ… Enabled real-time gesture classification for assistive communication.
